{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crypto Dataset Link https://pythonprogramming.net/static/downloads/machine-learning-data/crypto_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            BTC-USD_close  BTC-USD_volume  LTC-USD_close  LTC-USD_volume  \\\n",
      "time                                                                       \n",
      "1528968660    6489.549805        0.587100      96.580002        9.647200   \n",
      "1528968720    6487.379883        7.706374      96.660004      314.387024   \n",
      "1528968780    6479.410156        3.088252      96.570000       77.129799   \n",
      "1528968840    6479.410156        1.404100      96.500000        7.216067   \n",
      "1528968900    6479.979980        0.753000      96.389999      524.539978   \n",
      "\n",
      "            ETH-USD_close  ETH-USD_volume  BCH-USD_close  BCH-USD_volume  \n",
      "time                                                                      \n",
      "1528968660            NaN             NaN     871.719971        5.675361  \n",
      "1528968720      486.01001       26.019083     870.859985       26.856577  \n",
      "1528968780      486.00000        8.449400     870.099976        1.124300  \n",
      "1528968840      485.75000       26.994646     870.789978        1.749862  \n",
      "1528968900      486.00000       77.355759     870.000000        1.680500  \n"
     ]
    }
   ],
   "source": [
    "data_root = r\"D:/Research/LSTM/Sample_Data/crypto_data\"\n",
    "main_df = pd.DataFrame()\n",
    "ratios  = [\"BTC-USD\", \"LTC-USD\", \"ETH-USD\", \"BCH-USD\"]\n",
    "\n",
    "for ratio in ratios:\n",
    "    data_path = data_root+\"/\"+ratio+\".csv\"\n",
    "    df = pd.read_csv(data_path, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\n",
    "    \n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    df = df[[f\"{ratio}_close\", f\"{ratio}_volume\"]]\n",
    "    \n",
    "    if len(main_df) == 0:\n",
    "        main_df = df\n",
    "    else:\n",
    "        main_df = main_df.join(df)\n",
    "\n",
    "print(main_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting Constants\n",
    "# Problem Statement: Take last 60 minutes of pricing and predict the next 3 minutes\n",
    "SEQ_LEN = 60\n",
    "FUTURE_PERIOD_PREDICT = 3 # 3 minutes\n",
    "RATIO_TO_PREDICT = \"LTC-USD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label data\n",
    "def classify(current, future):\n",
    "    if float(future) > float(current):\n",
    "        return 1 # If the price is higher in the future than it is now (present)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new column future contains the values present in corresponding RATIO_TO_PREDICT row below f\"{RATIO_TO_PREDICT}_close\"\n",
    "# Example: 1st rwo in future == 4th row in  f\"{RATIO_TO_PREDICT}_close\n",
    "# Example: 2nd rwo in future == 5th row in  f\"{RATIO_TO_PREDICT}_close\n",
    "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the target label\n",
    "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\n",
    "main_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot shuffle and take a random 10% because the sequences are 60 min long and we are predicting the three minutes.\n",
    "# if we shuffle and draw 10%, the samples would all have close examples making easy the rnn model to overfit fully.\n",
    "# Instead, take a chunk and seperate them away\n",
    "# For a time series data, take a chunk of data in the future\n",
    "# In our data, seperate the last 5% as the out of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validation split\n",
    "times = sorted(main_df.index.values)\n",
    "last_5_percent = times[-int(0.05*len(times))] # indexing the last 5 percent of the times\n",
    "validation_main_df = main_df[(main_df.index >= last_5_percent)]\n",
    "train_main_df = main_df[(main_df.index < last_5_percent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df = df.drop('future', 1) # we only require the \"future\" column for labelling, so dropping it.\n",
    "    for col in df.columns:\n",
    "        if col != 'target':\n",
    "            df[col] = df[col].pct_change() # Normalising the column values\n",
    "            df.dropna(inplace=True) # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values) # scaling - making all values in the column between 0 and 1\n",
    "    df.dropna(inplace=True)\n",
    "    sequential_data = [] # this is a list that will CONTAIN the sequences\n",
    "    prev_days = deque(maxlen=SEQ_LEN)\n",
    "    # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in    prev_days = deque(maxlen=SEQ_LEN) \n",
    "    for i in df.values:\n",
    "        prev_days.append([n for n in i[:-1]]) # # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN: # make sure we have 60 sequences!\n",
    "            sequential_data.append([np.asarray(prev_days), i[-1]])\n",
    "    random.shuffle(sequential_data)\n",
    "    # balancing the data to improve model performance\n",
    "    buys = []\n",
    "    sells = []\n",
    "    for seq, target in sequential_data:\n",
    "        if target == 0:\n",
    "            sells.append([seq, target])\n",
    "        else:\n",
    "            buys.append([seq, target])\n",
    "    random.shuffle(buys)\n",
    "    random.shuffle(sells)\n",
    "    lower = min(len(buys), len(sells))\n",
    "    buys = buys[:lower]\n",
    "    sells = sells[:lower]\n",
    "    sequential_data = buys+sells\n",
    "    random.shuffle(sequential_data)\n",
    "    # splitting into x (features) and y (labels)\n",
    "    X = []\n",
    "    y = []\n",
    "    for seq, target in sequential_data:\n",
    "        X.append(seq)\n",
    "        y.append(int(target))\n",
    "    return np.array(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = preprocess_df(train_main_df)\n",
    "val_x, val_y = preprocess_df(validation_main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: 68836 validation: 3400\n",
      "Dont buys: 34418, buys: 34418\n",
      "VALIDATION Dont buys: 1700, buys: 1700\n"
     ]
    }
   ],
   "source": [
    "print(f\"train data: {len(train_x)} validation: {len(val_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {val_y.count(0)}, buys: {val_y.count(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "NAME = f\"{RATIO_TO_PREDICT}-{SEQ_LEN}-SEQ-{FUTURE_PERIOD_PREDICT}-PRED-{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\fkmkw1\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 68836 samples, validate on 3400 samples\n",
      "Epoch 1/10\n",
      "68836/68836 [==============================] - 72s 1ms/sample - loss: 0.6978 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "68836/68836 [==============================] - 74s 1ms/sample - loss: 0.6936 - acc: 0.4974 - val_loss: 0.7209 - val_acc: 0.5012\n",
      "Epoch 3/10\n",
      "68836/68836 [==============================] - 74s 1ms/sample - loss: 0.6936 - acc: 0.4977 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "68836/68836 [==============================] - 75s 1ms/sample - loss: 0.6936 - acc: 0.4977 - val_loss: 0.6947 - val_acc: 0.4982\n",
      "Epoch 5/10\n",
      "68836/68836 [==============================] - 75s 1ms/sample - loss: 0.6935 - acc: 0.4968 - val_loss: 0.6950 - val_acc: 0.4997\n",
      "Epoch 6/10\n",
      "68836/68836 [==============================] - 75s 1ms/sample - loss: 0.6934 - acc: 0.5017 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "68836/68836 [==============================] - 77s 1ms/sample - loss: 0.6935 - acc: 0.4990 - val_loss: 0.6956 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "68836/68836 [==============================] - 77s 1ms/sample - loss: 0.6934 - acc: 0.5036 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "68836/68836 [==============================] - 76s 1ms/sample - loss: 0.6935 - acc: 0.4978 - val_loss: 0.6954 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "68836/68836 [==============================] - 75s 1ms/sample - loss: 0.6934 - acc: 0.4994 - val_loss: 0.6995 - val_acc: 0.4994\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128)) # no return sequences as next layer is a Dense layer\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir = f'logs\\{NAME}')\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "history = model.fit(\n",
    "    train_x, np.array(train_y),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
